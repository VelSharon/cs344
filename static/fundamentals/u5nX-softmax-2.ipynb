{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `008-softmax-2`\n",
    "\n",
    "Task: more practice using the `softmax` function, and connect it with the `sigmoid` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this example: `x = tensor([0.1, 0.2, 0.3])`\n",
    "\n",
    "1. If `p=softmax(x)`, what is `p.sum()`? Can you get `p.sum()` to change by changing `x`? Can you make `p.min()` be less than 0?\n",
    "2. Try making an `xx` that's the same as `x` except that `xx[2] = 100`, and let `p = softmax(xx)` again. How does `p[2]` compare with `p[0]` and `p[1]`?\n",
    "3. Try `torch.sigmoid(tensor(0.1))`. Can you write an expression that uses `torch.softmax` to get the same output?\n",
    "\n",
    "**Hint for \\#3**: Give `sigmoid` a two-element `tensor`. One of those elements will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Add code and Markdown cells for each of the listed tasks above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sometimes `x` is called the \"logits\" and `x.softmax().log()` (or `x.log_softmax()`) is called the \"logprobs\", short for \"log probabilities\". Compute the logits, logprobs, and probabilities for `x` in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In light of your observations about `xx[2]` and `p[2]` above, why might `softmax` be an appropriate name for this function?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
